{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_1 = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\plate1.csv\"\n",
    "plate_1_repeat = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\plate1_repeat.csv\"\n",
    "plate_2_1 = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\plate2_1.csv\"\n",
    "plate_2_2 = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\plate2_2.csv\"\n",
    "plate_2_3 = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\plate2_3.csv\"\n",
    "plate_2_repeat = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\plate2_repeat.csv\"\n",
    "plate_2_repeat_96 = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\plate2_repeat _96.csv\"\n",
    "list_A = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\listA.csv\"\n",
    "list_A_repeat = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\listA_repeat.csv\"\n",
    "list_B = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\listB.csv\"\n",
    "list_B_repeat_end = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\listB_repeat _end.csv\"\n",
    "list_B_repeat_96 = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\listB_repeat _96.csv\"\n",
    "list_C = \"C:\\\\Users\\\\Bartek\\\\OneDrive\\\\Documents\\\\Programming\\\\Python\\\\Test data\\\\listC.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_dimensions = {'96':(8, 12), '384':(16, 24)}\n",
    "\n",
    "\n",
    "class Error(Exception):\n",
    "    pass\n",
    "\n",
    "class DataReadInError(Error):\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_in_plate(csv_file):\n",
    "    \n",
    "    \"\"\" Iterates over the raw data file to find the line numbers at which the metadata table and raw data table begin for \n",
    "    each channel and reapeat. Calculates the length of those tables. Finds the G-factor. Determines the format of data (plate 1 or 2).\n",
    "    \n",
    "    :param csv_file: Raw data file in csv format.\n",
    "    :param type: str\n",
    "    :return: A tuple containing a list of tuples (one tuple for each channel) and a string representing data format. \"\"\"\n",
    "    \n",
    "    with open(csv_file) as file:\n",
    "        all_data_lines = list(csv.reader(file, delimiter=','))   # read the csv file and cast it into a list containing all lines\n",
    "        \n",
    "    blank_indices = list(index for index, item in enumerate(all_data_lines) if item == [])   # list containing indices of all blank rows\n",
    "    blanks = np.array(blank_indices)\n",
    "    values = []   # list for storage of tuples\n",
    "\n",
    "    for index, item in enumerate(all_data_lines):   # iterate over each line of the csv file\n",
    "        if item != [] and re.findall(r\"Plate information\", item[0]) == ['Plate information'] and re.search(r'Results for', all_data_lines[index + 9][0]) == None and re.findall(r\"Formula\", all_data_lines[index+1][10]) != ['Formula']:\n",
    "            skiprows = index + 9   # Set the skiprows parameter for raw data table\n",
    "            skiprows_meta = index + 1   # Set the skiprows parameter for metadata table\n",
    "            end_of_data = blanks[blanks > skiprows].min()   # calculate the end of data table by finding the smallest blank index after the beginning of data table\n",
    "            values.append((skiprows, end_of_data-skiprows+1, skiprows_meta))   # add the skiprows, caculated number of data lines and skiprows for metadata parameters to the list as a tuple\n",
    "            data_format = 'plate1'\n",
    "        \n",
    "        if item != [] and re.findall(r\"Plate information\", item[0]) == ['Plate information'] and re.search(r'Results for', all_data_lines[index + 9][0]) != None:\n",
    "            skiprows = index + 10\n",
    "            skiprows_meta = index + 1\n",
    "            end_of_data = blanks[blanks > skiprows].min()\n",
    "            values.append((skiprows, end_of_data-skiprows, skiprows_meta))\n",
    "            data_format = 'plate2'\n",
    "\n",
    "        if item != [] and len(item) > 1 and re.findall(r\"G-factor\", item[0]) == [\"G-factor\"]:\n",
    "            g_factor = float(item[4])   \n",
    "                \n",
    "    return values, data_format\n",
    "\n",
    "\n",
    "def process_plate(csv_file, values, wells):    \n",
    "    \n",
    "    \"\"\" Iterates over the raw data file and creates data frames for the data and metadata for each channel, converts them into\n",
    "    a 384 or 96 by 1 format and adds them into a dictionary.\n",
    "    \n",
    "    :param csv file: Raw data file in csv format.\n",
    "    :param type: str\n",
    "    :param values: A list containg tuples with read in parameters for each channel and the data format parameter.\n",
    "    :param type: tuple\n",
    "    :param wells: Number of wells on the plate.\n",
    "    :param type: str\n",
    "    :return: A dictionary containg a dictionary for each repeat containg the metadata df and a dictionary with s and p channel dfs. \"\"\"\n",
    "    \n",
    "    plate_dimensions = {'96':(8, 12), '384':(16, 24)}   \n",
    "\n",
    "    row_letters = list(string.ascii_uppercase)[0:plate_dimensions[wells][0]]   # generate letters for the data table\n",
    "    col_numbers = list(np.arange(1, plate_dimensions[wells][1]+1).astype(str))   # generate numbers for the data table\n",
    "    well_ids = ['%s%s' % (item[0], item[1]) for item in product(row_letters, col_numbers)]   # generate well IDs for the data table\n",
    "\n",
    "    data_frames = {}   # dictionary to store the data frames\n",
    "    counter = 1   # counter to enable alternating labelling of data frames as p or s\n",
    "    \n",
    "    for index,item in enumerate(values[0]):   # iterate over each tuple in the list\n",
    "        \n",
    "        if values[1] == 'plate1':   # raw data table does not have row and column names so 'names' parameter passed to omit the last column\n",
    "            raw_data = pd.read_csv(csv_file, sep=',', names=col_numbers, index_col=False, engine='python', skiprows=item[0], nrows=item[1], encoding='utf-8')\n",
    "        \n",
    "        if values[1] == 'plate2':   # raw data table has row an column names, so 'index_col' must be 0 \n",
    "            raw_data = pd.read_csv(csv_file, sep=',', index_col=0, engine='python', skiprows=item[0], nrows=item[1], encoding='utf-8')\n",
    "            raw_data.drop(raw_data.columns[-1], axis=1, inplace=True)   # delete the last column because it is empty\n",
    "\n",
    "        # generate df for metadata (number of rows is always 1) and conver measurement time into datetime object   \n",
    "        metadata = pd.read_csv(csv_file, sep=',', engine='python', skiprows=item[2], nrows=1, encoding='utf-8').astype({'Measurement date': 'datetime64[ns]'})\n",
    "        data_to_array = np.reshape(raw_data.to_numpy(), (int(wells), 1))   # convert data frames to numpy arrays and reshape into 1D array\n",
    "\n",
    "        if counter % 1 == 0: \n",
    "            new_data = pd.DataFrame(data=data_to_array, index=well_ids, columns=['p'])   # generate new 384 (or 96) by 1 data frame with p channel data\n",
    "            data_frames[f'repeat_{int(counter)}'] = {'metadata':metadata, 'data': {'p': new_data, 's':''}}   # add data and metadata dfs to the dictionary\n",
    "        \n",
    "        if counter % 1 != 0:\n",
    "            new_data = pd.DataFrame(data=data_to_array, index=well_ids, columns=['s'])   # generate new 384 (or 96) by 1 data frame with s channel data\n",
    "            data_frames[f'repeat_{int(counter-0.5)}']['data']['s'] = new_data\n",
    "\n",
    "        counter = counter + 0.5\n",
    "        \n",
    "    return data_frames\n",
    "\n",
    "\n",
    "def read_in_list(csv_file):\n",
    "\n",
    "    \"\"\" Iterates over the csv file to find  the line numbers at which the metadata table and raw data table\n",
    "    begin. Creates two pandas data frames: for the raw data and metadata.\n",
    "    \n",
    "    :param csv_file: Raw data file in csv format.\n",
    "    :param type: str\n",
    "    :return: A tuple with raw data and metadata dfs.\"\"\"\n",
    "    \n",
    "    with open(csv_file) as file:  \n",
    "        all_data_lines = list(csv.reader(file, delimiter=',')) # read the csv file and cast it into a list\n",
    "    \n",
    "    # set the skiprows to be greater than the total number of lines in the files to enable the evaluation of if statement until the 'skiprows' parameter is found\n",
    "    skiprows = len(all_data_lines) + 1 \n",
    "    # list containing indices of all blank rows\n",
    "    blank_indices = list(index for index, item in enumerate(all_data_lines) if item == [])   # list containing indices of all blank rows\n",
    "    blanks = np.array(blank_indices)   \n",
    "    \n",
    "    # iterate over all lines to find the beggining of the data table ('skiprows') and determine the format of data i.e. list A, B, or C\n",
    "    for index, item in enumerate(all_data_lines):   \n",
    "        if item != [] and len(item) == 1 and re.findall(r\"Plate information\", item[0]) == [\"Plate information\"]:\n",
    "            skiprows_meta = index + 1\n",
    "            end_of_metadata = blanks[blanks > skiprows_meta].min()   # find the end of metadata by finding the smallest blank index after the beginning of metadata\n",
    "            nrows_meta = end_of_metadata - skiprows_meta - 1   # calucalte the length of metadata table\n",
    "            \n",
    "        if item != [] and len(item) >= 2 and re.findall(r\"PlateNumber\", item[0]) == ['PlateNumber'] and re.findall(r\"PlateRepeat\", item[1]) == ['PlateRepeat']:   # find line number with the beggining of the data\n",
    "            skiprows = index - 1\n",
    "            data_format = 'listA'\n",
    "\n",
    "        if item != [] and len(item) >= 2 and re.findall(r\"Plate\", item[0]) == ['Plate'] and re.findall(r\"Barcode\", item[1]) == ['Barcode']:   # find line number with the beggining of the data\n",
    "            skiprows = index\n",
    "            data_format = 'listB'\n",
    "\n",
    "        if item != [] and len(item) >= 2 and re.findall(r\"Plate\", item[0]) == ['Plate']  and re.findall(r\"Well\", item[1]) == ['Well']:\n",
    "            skiprows = index\n",
    "            data_format = 'listC'\n",
    "\n",
    "        if item != [] and re.findall(r\"G-factor\", item[0]) == [\"G-factor\"]:   # find the g factor\n",
    "            g_factor = float(item[4])\n",
    "        \n",
    "        # find the index of the first blank row after the data table i.e. the end of data table and break out of the loop\n",
    "        if item == [] and index > skiprows:   \n",
    "            end_of_data = index\n",
    "            break  \n",
    "\n",
    "    nrows = end_of_data - skiprows - 1   # calculate the length of data table\n",
    "\n",
    "    raw_data = pd.read_csv(csv_file, sep=',', engine='python', skiprows=skiprows, nrows=nrows, encoding='utf-8')\n",
    "    metadata = pd.read_csv(csv_file, sep=',', engine='python', skiprows=skiprows_meta, nrows=nrows_meta, encoding='utf-8')\n",
    "\n",
    "    return raw_data, metadata, data_format\n",
    "\n",
    "\n",
    "def process_list(csv_file, raw_data):\n",
    "    \n",
    "    \"\"\"Extracts the data for each channel and repeat from the raw data table and adds to a dictionary.\n",
    "    \n",
    "    :param csv_file: Raw data file in csv format.\n",
    "    :param type: str\n",
    "    :param  raw_data: A tuple containing data frames for raw data, metadata and astring representing type of list (A, B, or C).\n",
    "    :param type: tuple\n",
    "    :return: A dictionary containg a dictionary for each repeat containg the metadata df and a dictionary with s and p channel dfs.\"\"\"\n",
    "    \n",
    "    data_frames = {}   # dictionary to store data frames\n",
    "    repeats = list(raw_data[1]['Repeat'].to_numpy())   # generate a list with repeats based on the metadata table\n",
    "    \n",
    "    # remove the 0 from middle position of well numbers (i.e. convert A01 to A1)\n",
    "    for i in range(raw_data[0].shape[0]):\n",
    "        if raw_data[0].loc[i, 'Well'][2] != '0':\n",
    "            raw_data[0].replace(raw_data[0].loc[i, 'Well'], raw_data[0].loc[i, 'Well'].replace('0', ''), inplace=True)\n",
    "    \n",
    "    for index, repeat in enumerate(repeats):   # iterate over the number of repeats\n",
    "    \n",
    "        if raw_data[2] == 'listA':\n",
    "            groupped_data = raw_data[0].groupby(raw_data[0].PlateRepeat).get_group(repeat)   # group and extract the data by the plate repeat column, i.e. in each iteration get data only for the current repeat \n",
    "            \n",
    "            p_groupped = groupped_data.iloc[::3, :]   # extract data only for the p channel, i.e. each third row starting from the first row\n",
    "            s_groupped = groupped_data.iloc[1::3, :]   # extract data only for the s channel, i.e. each third row starting from the second row\n",
    "\n",
    "            p_raw = p_groupped[['Well', 'Signal']]   # extract only the two relevant columns\n",
    "            s_raw = s_groupped[['Well', 'Signal']]   # for each channel\n",
    "            \n",
    "        if raw_data[2] == 'listB' or raw_data[2] == 'listC': \n",
    "            # the column naming is different for the first repeat, i.e. just 'Signal', then it's 'Signal.1', 'Signal.2', etc.\n",
    "            if repeat == 1: \n",
    "                p_raw = raw_data[0][['Well', 'Signal']]   \n",
    "                s_raw = raw_data[0][['Well', f'Signal.{repeat}']]\n",
    "            else:\n",
    "                p_raw = raw_data[0][['Well', f'Signal.{repeat + index - 1}']]   # the column to be extracted is calculated in each iteration\n",
    "                s_raw = raw_data[0][['Well', f'Signal.{repeat + index}']]\n",
    "        \n",
    "        # set row indices as the well numbers and rename the 'Signal' column to 'p' or 's'\n",
    "        p_raw.set_index('Well', inplace=True)\n",
    "        p_raw.set_axis(['p'], axis=1, inplace=True)\n",
    "        s_raw.set_index('Well', inplace=True)\n",
    "        s_raw.set_axis(['s'], axis=1, inplace=True)\n",
    "        \n",
    "        meta = raw_data[1].iloc[[repeat-1]].astype({'Measurement date': 'datetime64[ns]'})   # extract the row with metadata relevant for each repeat and covert date and time into a datetime object\n",
    "        data_frames[f'repeat_{repeat}'] = {'metadata': meta, 'data': {'p': p_raw, 's':s_raw}}   # add data frames to the dictionary\n",
    "        \n",
    "    return data_frames\n",
    "\n",
    "\n",
    "class FA:\n",
    "    \n",
    "    \"\"\"\n",
    "    :param csv_file: A csv file containing raw data.\n",
    "    :param type: str\n",
    "    :param size: Number of wells on the plate.\n",
    "    :param type: int\n",
    "    :param data_type: Format in which the raw data was exported (plate1, plate2, listA, listB or listC)\n",
    "    :param type: str \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, size, data_type):\n",
    "        self.csv_file = csv_file\n",
    "        self.size = size\n",
    "        self.data_type = data_type\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
